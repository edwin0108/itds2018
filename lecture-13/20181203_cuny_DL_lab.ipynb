{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT63quvgri0L"
   },
   "source": [
    "* Live edited version of the ipynb:\n",
    "https://colab.research.google.com/drive/1wLjS2MeHaJDcqZ6zQOfb7TlQ4uBt9L81\n",
    "* The empty ipynb for you to start from in Grant's repo: https://github.com/grantmlong/itds2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTApi-7voeCo"
   },
   "source": [
    "# Setting the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fP6QeXnVeWzi",
    "outputId": "4b1a109d-f35d-4a33-b758-bff725a5dcf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) \\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "fHza7chYecM4",
    "outputId": "0ec0e972-728b-447b-a5ba-91a9bbe9dada"
   },
   "outputs": [],
   "source": [
    "# if running on colab, we'll install pytorch right here right now\n",
    "# if running locally, pip install this in your conda environment.\n",
    "# note we're installing the bleeding edge 1.0.0 pre-release, official release coming soon.\n",
    "# !pip install torchvision\n",
    "# !pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ropMkTLTiWos",
    "outputId": "9cba56ae-1b3c-4bd1-f85f-f9c7e83fa9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0.dev20181130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgfEx8YN0BDF"
   },
   "source": [
    "# Torch and autograd basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is a package that defines vectors, matrices, or in general \"tensors\". If you know numpy, you will not be surprised by any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmvFvWHJ0GQ1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mur2vsHp0GmX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(9).float().view(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hQiSHvW0G2v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.],\n",
       "        [49., 64., 81.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a+b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 3., 6.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_() # operations with an underscore modify the Tensor in place.\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can slice and dice tensors and they have roughly all tensor operations you expect equivalently to numpy, but with a bit more low level control. If you need more intro: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "So what's the big deal about pytorch?\n",
    "\n",
    "**autograd = automatic differentiation**.\n",
    "\n",
    "Every `torch.Tensor`, let's say `x`, has an important flag `requires_grad`. If this flag is set to True, pytorch will keep track of the graph of operations that happen with this tensor.\n",
    "When we finally arrive at some output (a scalar variable based on a sequence of operations on `x`), we can call `.backward()` on this output, to compute the gradient `d(output) / dx`. This gradient will end up in `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8373,  0.5334],\n",
       "        [-1.1184,  0.1904]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7154, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=(x**2 + x)\n",
    "z = y.sum()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from high school math that the derivative `dz / dx[i,j]` = 2*x +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6746,  2.0669],\n",
       "        [-1.2369,  1.3807]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6746,  2.0669],\n",
       "        [-1.2369,  1.3807]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the intermediate variable y? Does it require a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the gradient of y is not exposed, since it is an intermediary variable, the result of an operation on leaf variables. Leaf variables are inputs to the operations: the data X or the `Parameter`s of a neural network.\n",
    "\n",
    "More about autograd in the tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py and the docs https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we talked about how derivatives and backpropagation (based on the chain rule of differentiation) play a central role in deep learning. You can now start to see how this autograd will be massively powerful to define neural networks with weights `W` and optimize them based on the gradients in `W.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for a simple linear mapping `y = W x `, where we want to optimize W:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3611, 0.1379]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(23801)\n",
    "x = torch.Tensor([ [1, 0], [2, 0], [3, 0], [-3, 0], [-2, 0] ]) + 0.3*torch.randn(5,2)\n",
    "y = torch.Tensor([3, 6, 9, -9, -6])\n",
    "# clearly the right relationship is y  = 3*x[:,0] + 0 * x[:,1].  So we can see that the optimal W = [3,0]\n",
    "W = torch.randn(1,2, requires_grad=True)\n",
    "# we start W at random initialization, the gradient will point us in the right direction.\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9376,  5.6538,  9.5196, -8.3741, -6.5541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([[0.0003, 0.0092]])\n",
      "tensor([[ 2.7222, -0.0378]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "ypred = x @ W.t()\n",
    "print(ypred.squeeze())\n",
    "loss = ((ypred.squeeze()-y)**2).mean() # mean squared error.\n",
    "loss += 0.1 * W.norm() # stabilization term leading to weight decay\n",
    "loss.backward()\n",
    "print(W.grad)\n",
    "# let's move W in that direction\n",
    "W.data -= 0.1 * W.grad.data\n",
    "W.grad.data.zero_()\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can re-execute this cell above a couple of times and see how W oscillates around the optimal value of `[3,0]`.\n",
    "\n",
    "torch defines `Module`s which do two things: (a) they contain the learnable weight, and (b) define how they operate on an input tensor to give an output.\n",
    "In this case this would be a `Linear` layer, reducing 2D datapoints `x` to 1D output `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2608],\n",
       "        [ 0.7418],\n",
       "        [ 1.3510],\n",
       "        [-1.2409],\n",
       "        [-0.9287]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(2,1, bias=False)\n",
    "linear.weight.data.copy_(W) # we re-initialize the linear layer with the W we just found\n",
    "ypred = linear(x)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could do the same thing, compute MSE loss and backprop, then update the linear layer's weight which you can access like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3917, 0.1368]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0y1lGv8oIHI"
   },
   "source": [
    "# Revisiting KIVA logistic regression from lab 6/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSygYcyKwTf6"
   },
   "source": [
    "## sklearn solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "tx6mA3tJo3DE",
    "outputId": "e70a0c66-f484-4b05-ce71-01a0dd9344c3"
   },
   "outputs": [],
   "source": [
    "# if you downloaded the kiva data locally, you can skip wget and just set the path\n",
    "#!wget https://grantmlong.com/data/kiva_kenya_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "72VlJgMjo3v9",
    "outputId": "af4e9a1f-d6b5-433a-d047-28235ebf50dc"
   },
   "outputs": [],
   "source": [
    "# So here i copy pasted the lab 6/7 logistic regression code where we leave the\n",
    "# model fitting to sklearn.\n",
    "data_path = 'kiva_kenya_sample.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "print(list(df))\n",
    "df['success'] = (df.STATUS=='funded')*1\n",
    "df['posted_year'] = pd.to_datetime(df.POSTED_TIME).dt.year\n",
    "df['posted_duration'] = (pd.to_datetime(df.PLANNED_EXPIRATION_TIME)\n",
    "                             - pd.to_datetime(df.POSTED_TIME)\n",
    "                            ).dt.days\n",
    "model_columns = ['LOAN_AMOUNT', 'posted_year', 'posted_duration', 'LENDER_TERM']\n",
    "valids = df[model_columns].notna().all(axis=1)\n",
    "X = df.loc[valids, model_columns].values\n",
    "X_scaled = preprocessing.scale(X) # zero mean, unit variance\n",
    "\n",
    "y = df.loc[valids, 'success'].values\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=1235)\n",
    "\n",
    "# OK data is ready, set up the logistic regression classifier and train it.\n",
    "clf = LogisticRegression(random_state=20181022, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "W, b = clf.coef_, clf.intercept_\n",
    "print('The train accuracy of the sklearn model is %0.1f%%' % (clf.score(X_train_np, y_train_np)*100))\n",
    "print('The test  accuracy of the sklearn model is %0.1f%%' % (clf.score(X_test_np, y_test_np)*100))\n",
    "print('The learned model: y=W*x+b where \\nW={} and \\nb={}'.format(W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJwW0C5Yw1Rg"
   },
   "source": [
    "* How many parameters (weights) does our logistic regression model have? How many  datapoints did we train on?\n",
    "* Old-skool machine learning rule of thumb is: you can optimize about as many parameters (weights) as you have datapoints before you can memorize the dataset (thus overfit heavily). Are we close to the limit?\n",
    "* Does the model overfit?\n",
    "* In deep neural networks you can easily have way more parameters than datapoints. Is overfitting an issue for neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2-BhVGTvOTM"
   },
   "outputs": [],
   "source": [
    "# number of parameters is defined by weight and bias dimensionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgYdSOCXwc0j"
   },
   "source": [
    "## torch version of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaHzzHci0m5i"
   },
   "source": [
    "Ok so above we used the sklearn implementation for logistic regression, which optimizes  $$\\mathcal{L(W,b)} = \\sum_{x,y_{t} \\in D} \\ell(x,y_t; W,b)$$\n",
    "with $$\\ell(x,y_t; W,b) = \\text{CE}(\\sigma(Wx+b) || y_t)$$\n",
    "So per sample, the sigmoid of the linear transformation $\\sigma(Wx+b)$ is the model's prediction of the probability.  This is being compared to the true label $y_t$,\n",
    "by the Cross-Entropy loss: $\\text{CE}(\\sigma(Wx+b) || y_t)$.\n",
    "\n",
    "Now we'll set up a small neural network, and optimize the same loss with SGD. We will compare the solution to the one found by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the typical training procedure for a neural network which is as follows:\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zr4H1--I3ohW"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = torch.from_numpy(X_train_np).float(), torch.from_numpy(X_test_np).float()\n",
    "y_train, y_test = torch.from_numpy(y_train_np).int(),   torch.from_numpy(y_test_np).int()\n",
    "# we will define a \"neural network\" of 1 layer:\n",
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # computes W X + b where X is a (batch of) 4D vectors and y will be 1D.\n",
    "sigmoid = nn.Sigmoid()\n",
    "loss = nn.BCELoss() # Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhT6DQenyJOE"
   },
   "source": [
    "Print the value of the weight and the bias of the network. Do they have any gradients right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_n2F1zuzyJUG"
   },
   "outputs": [],
   "source": [
    "# For most modules, they're called .weight and .bias\n",
    "# gradients are an attribute of a Parameter (weight) and are accessible in .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "S851ry0z4Mls",
    "outputId": "50ebecd1-b169-4b15-a7a3-0012dc8f01a0"
   },
   "outputs": [],
   "source": [
    "def print_accs(net, s=''):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    # net(x) is continuous value, threshold at 0 to make binary prediction\n",
    "    pred_train = (net(X_train) > 0).squeeze().int()\n",
    "    acc_train = (pred_train == y_train).float().mean()\n",
    "    print('Train accuracy is %0.1f%%  - %s' % (acc_train*100, s))\n",
    "    net.eval()\n",
    "    pred_test = (net(X_test) > 0).squeeze().int()\n",
    "    acc_test = (pred_test == y_test).float().mean()\n",
    "    print('Test  accuracy is %0.1f%%  - %s' % (acc_test*100, s))\n",
    "    net.train()\n",
    "# let's print the accuracies for the untrained net, which will be random guess\n",
    "print_accs(net, 'just initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hides everything from you. Let's copy the weights into a torch network\n",
    "# and compute accuracies ourselves.\n",
    "refnet = nn.Linear(4,1)\n",
    "W, b = clf.coef_, clf.intercept_\n",
    "refnet.weight.data.copy_(torch.from_numpy(W))\n",
    "refnet.bias.data.copy_(torch.from_numpy(b))\n",
    "print(refnet.weight) # from the sklearn optimized model\n",
    "print_accs(refnet, 'sklearn weights copied over into refnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNjVlTt87ou1"
   },
   "source": [
    "Now we'll optimize the weights and bias with SGD. Dataset and DataLoader are abstractions to help us iterate over the data in random order. We will do 1 epoch, i.e. we go through the data only once, in minibatches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nIhpH0sN7o5m",
    "outputId": "2c3207e0-8da8-4053-ac73-17fe5eef07f2"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # re initialize the net from scratch\n",
    "print('Just initialized: weight: ', net.weight, '\\nbias: ', net.bias)\n",
    "w, b = net.weight, net.bias \n",
    "lr = 1.0\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1)) # forward, pytorch constructs the graph\n",
    "    output = loss(ypred, ytarget.float()) # forward, pytorch constructs the graph\n",
    "    output.backward() # backward, pytorch computes net.weight.grad and net.bias.grad\n",
    "    # access to the weight & bias tensors outside of pytorch autograd\n",
    "    w, grad_w, b, grad_b = net.weight.data, net.weight.grad.data, net.bias.data, net.bias.grad.data\n",
    "    # TODO: do an SGD step for both weight and bias: w -= lr * grad\n",
    "    # TODO: manually clear the gradient of the weight and the bias (use `.zero_()` method) \n",
    "    # we need to do this before doing the forward and backward pass.\n",
    "print_accs(net, 'after epoch')\n",
    "print('weight: ', net.weight, '\\nbias: ', net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w00A5MF07pIw"
   },
   "source": [
    "Ok doing this manually gives you insight what happens down to the gradienst. But usually we do not do these things manually, it would become very cumbersome if the net becomes more complex than the simple linear layer. pytorch gives us primitives to do the same: `net.zero_grad()` to clear the gradients, and for optimization you can do `optimizer.step()` to do a step of SGD.\n",
    "Again we will do 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oxs6aggO7pQH"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # re initialize the net from scratch\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1))\n",
    "    output = loss(ypred, ytarget.float())\n",
    "    output.backward()\n",
    "    # TODO use the gradients to do a step (use the optimizer)\n",
    "    # TODO clear the gradient\n",
    "print_accs(net, 'after epoch')\n",
    "print('weight: ', net.weight, '\\nbias: ', net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tBv23he7pXP"
   },
   "source": [
    "Ok now let us redo this but for a real 3-layer neural network. You can re-execute the cell a couple of times to do more iterations and see the accuracy improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1248)\n",
    "net = nn.Sequential(\n",
    "    # first layer\n",
    "    nn.Linear(4,32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    # output layer going to 1 prediction\n",
    "    nn.Linear(32,1),\n",
    ")\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ajv2kGVU7peF"
   },
   "outputs": [],
   "source": [
    "# Do 1 epoch:\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1))\n",
    "    output = loss(ypred, ytarget.float())\n",
    "    output.backward()\n",
    "    optimizer.step()\n",
    "    net.zero_grad()\n",
    "print_accs(net, 'after epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the final real deal, we train for 10 epochs and cut the learning rate in half between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-6-LmwMufPD"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1248)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,16),\n",
    "#     nn.Dropout(0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,32),\n",
    "#     nn.Dropout(0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,16),\n",
    "    nn.ReLU(),\n",
    "    # output layer going to 1 prediction\n",
    "    nn.Linear(16,1),\n",
    ")\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'Before training')\n",
    "for epoch in range(10):\n",
    "    scheduler.step()\n",
    "    for x,ytarget in dl:\n",
    "        ypred = sigmoid(net(x).squeeze(1))\n",
    "        output = loss(ypred, ytarget.float())\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "    print_accs(net, 'After epoch {}'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila and that's how it's done. You can ask yourself some more questions:\n",
    "* What do we get back from `net.parameters()`: which trainable weights and biases does the network have now? \n",
    "* How many total parameters? \n",
    "* What happens if you add layers or change the ReLU activation by Sigmoid activation?\n",
    "* What does the Dropout layer do? \n",
    "\n",
    "There are many things you can play around with and where you can dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjVmBKaavPJP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdh_P4DKoNup"
   },
   "source": [
    "# Now the real stuff: MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of 50k handwritten digits (0-9) which is very commonly used in the deep learning community.\n",
    "It is small enough to work with locally and without much hassle, and complex enough to do something interesting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "PH8zthzBhjKS",
    "outputId": "d959308e-faa5-4912-a44e-d5be1001b65b"
   },
   "outputs": [],
   "source": [
    "# let's download the MNIST data, if you do this locally and you downloaded before,\n",
    "# you can change data paths to point to your existing files\n",
    "train_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the digits and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=129\n",
    "x,y = train_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "print('label: y={}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the dataloaders and train simple neural network like before.\n",
    "You'll recognize that the core is exactly the same: we do a forward pass, compute a loss, backpropagate the loss to compute the gradients, then let the optimizer update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tONXsS3FtvFK"
   },
   "outputs": [],
   "source": [
    "# The neural network hyperparameters.\n",
    "input_size    = 784   # The MNIST image size = 28 x 28 = 784\n",
    "hidden_size   = 100   # The number of nodes at the hidden layer\n",
    "num_classes   = 10    # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs    = 5     # The number of times entire dataset is trained\n",
    "batch_size    = 100   # The number of samples per minibatch\n",
    "learning_rate = 1.0   # SGD step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "UZoB9O6ce0js",
    "outputId": "b20332d2-7abd-42f5-f788-452591dccdde"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define simple MLP network. Train network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        x = x.view(x.size(0), -1) # flatten (bs x 1 x 28 x 28) -> (bs x 784)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "def test(net, dl):\n",
    "    right, tot = 0, 0\n",
    "    net.eval() # Set dropout and possibly other modules in eval mode.\n",
    "    for x,y in dl:\n",
    "        ypred = net(x).argmax(dim=1) # select index of maximal score\n",
    "        right += (ypred == y).sum().item()\n",
    "        tot   += x.size(0)\n",
    "    return 1.* right / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if on gpu-enabled machine, set torch.device('cuda')\n",
    "# create the net based on this class definition\n",
    "net = Net(input_size, hidden_size, num_classes).to(device)\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "print('Before training: {:.1f}% test accuracy'.format(100*test(net, test_loader)))\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (x,y_target) in enumerate(train_loader):\n",
    "        y_probs = F.softmax(net(x), dim=1)\n",
    "        output = F.nll_loss(y_probs, y_target)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "    print('After epoch {}: {:.1f}% test accuracy'.format(epoch, 100*test(net, test_loader)))\n",
    "print('End of training: {:.1f}% train accuracy'.format(100*test(net, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some questions for you to investigate:\n",
    "* what does softmax do? (test on a random vector)\n",
    "* what is its purpose? (read the docs)\n",
    "* what does nll_loss do? can you manually compute it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction on some samples.\n",
    "ix=1234\n",
    "x,y = test_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "print('Ground truth label: y={}'.format(y))\n",
    "y_probs = F.softmax(net(x), dim=1)\n",
    "print ('Model probabilities: ') \n",
    "print(' / '.join(['{}: {:.3f}'.format(k,v) \n",
    "                  for k,v in zip(range(10), y_probs.squeeze().tolist()) ] ))\n",
    "print('Model prediction: ', y_probs.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used a simple flat neural network which looks at the image as a flat vector, without awareness of the 2D structure or which pixels neighbor each other.\n",
    "A convolutional neural network is an architecture that takes the 2D structure of the image into account by sliding a kernel over all the different locations in the image. This kind of neural network has been very succesful in image recognition [1] and speech recognition [2,3].\n",
    "Pytorch and other deep learning toolboxes are designed to deal with this kind of data and with convolutional neural networks just as easily as with flat data. Try swapping out the network above for a convolutional neural network, see for example the pytorch tutorial [4].\n",
    "\n",
    "[1] https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "[2] http://www.cs.toronto.edu/~asamir/papers/icassp13_cnn.pdf\n",
    "[3] https://arxiv.org/abs/1509.08967\n",
    "[4] https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "[5] https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48JIxr6HoXH6"
   },
   "source": [
    "# Finishing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUsyDpADjBbI"
   },
   "source": [
    "Inspiration for this lab and the lecture:\n",
    "\n",
    "*  An old lab I made in lua torch https://github.com/tomsercu/torchtutorial\n",
    "* This pytorch intro notebook https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c\n",
    "* The official pytorch tutorial https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "* Yann LeCuns deep learning course in 2015 https://cilvr.nyu.edu/doku.php?id=deeplearning2015:schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJog1HK1nRf7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jdh_P4DKoNup",
    "48JIxr6HoXH6"
   ],
   "name": "20181203_cuny_DL_lab",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
